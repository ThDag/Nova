Intellivo AI
Alice.
November 01, 2023.
Product Overview  


Alice is a conversational chatbot I'm developing for psychological evaluations. This chatbot will be able to converse, evaluate and diagnose people with psychological trauma and either encourage them to see the psychologist or therapist closest to their location or provide advice and support to the user depending on the situation they're in. I'm looking to make Alice able to make phone calls when necessary and also maybe add augmented reality to make it seem like you're talking to an actual person through your mobile device, laptop, or VR goggles.
Inspiration
I decided to build this project back in 2020 while still in high school after watching a documentary on BBC about the number of mentally unstable people living in a particular area after study. It was stated that most of them knew they needed psychological evaluations but later discovered that they disregarded them because they found it uncomfortable speaking with a person or group of people about their problems. That made me come up with the concept of Alice!


Extended Description
Overview


Generally, a chatbot is software that simulates human-like conversations with users via text messages on chat. Its key task is to help users by providing answers to their questions. A psychological evaluation chatbot is a type of conversational AI system that uses natural language processing and machine learning algorithms to conduct a psychological evaluations on the user. The chatbot typically begins by asking the user a series of questions related to their mental health and well-being, such as their mood, stress levels, and coping mechanisms. Based on the user's responses, the chatbot may provide feedback and suggestions for improving the user's mental health, as well as referrals to mental health professionals if necessary.
Psychology defines personality as a set of patterns visible in thinking, feeling, and behaving that distinguish an individual from others. The personality represents itself through consistent and repeatable behavior. Overall, a psychological evaluation chatbot can provide a quick and convenient way for users to assess their mental health and receive suggestions for improvement. It can also help to reduce the stigma surrounding mental health by providing a non-judgmental, accessible way for users to seek help and support. We have researched and observed the concepts between human-computer interactions and psychology and decided to piece them together to create Alice.
Alice is an AI-powered psychological chatbot, specifically developed to understand people. The core of the bot is represented by a huge dataset of the most reliable personality tests. Another function of Alice makes it outstanding – it can understand and process human language when it comes to describing personality. So, if you ask the bot questions about yourself, it can make some predictions about your personality traits or mental health status.


Objectives
The purpose of this project is to develop a conversational chatbot to conduct an initial and continuous conversational psychological assessment interview. With this project, I intend to create a chatbot with guarantees and scientific rigor that will help in psychological assessment work. In addition, once the bot has been built, it will allow the possibility to study the differences between the interviews in which the user interacts with a chatbot (human-chatbot process) and the interviews in which the user interacts with a human psychologist (human-human process). This will allow us to understand in depth what forms of interaction are most effective in achieving a certain objective. Furthermore, It will serve to establish the basis for the development of future tools, consultations, and evaluations as well as to initiate a debate about the relevance and the necessary regulation of their use. Alice will be able to converse, evaluate and diagnose people with psychological trauma and encourage them either to see the psychologist or therapist closest to their location or provide advice and support to the user depending on the situation they're in.


Advantages
1. Accessibility:
First of all, Alice is going to help relieve barriers associated with the stigma of accessing psychological health services and geographical barriers in face-to-face counseling. It will provide instant and relevant information 24/7 and serve exceptionally well for those working unconventional shifts. People enjoy the anonymity offered by chatbots, as they may be more likely to disclose sensitive information to a chatbot than to a human therapist (according to a recent study). Alice is also going to be built with personalized responses. Personalization is tailoring your communication and services to users' needs and characteristics. It’s a way to offer your target audience a unique experience that can increase their satisfaction. Alice will personalize the user's behavior based on data collected about them, such as their demographics (age or nationality), interactions, and many more. She is going to be the closest thing a human can be, except without a human body.
2. Cost Savings:
Alice will also offer cost savings in travel expenses and telephone charges for patients. As for practitioners, they can focus on people in critical condition and work with health data uncovered by AI, which implies more efficiency. The only requirement is an internet connection and battery life and boom, Alice is good to go!




How It’ll Work (Features):
1. Relationship between the Psychologist-Chatbot and the User:
Before starting to create Alice, it was deemed necessary to consider what type of virtual agent we want to program and for what purpose. In Alice’s case, we will present the necessary elements to build a chatbot aimed at performing preliminary psychological assessment tasks, although it is true that the concepts and steps we will describe could be generalized to robots with similar functions. It is also necessary to consider the information that the chatbot will obtain from the users. In our case, given that psychologists work with confidential information, we must take maximum care of the processing of the data and its accessibility.
  

2. Objectives of the chatbot with initial psychological assessment functions:
In Alice’s case, it is not the client who asks the chatbot for a particular type of information (schedules, types of tickets, etc.) or requires specific actions (booking, buying, etc.). Still, instead, it is the chatbot that asks the client to later provide the relevant information of his or her case to a psychological clinic. 
  



To build a conversational assistant with psychological assessment functions, we must adequately specify the objectives of the interview to be able to transmit them to the tool through the flow of dialogue and entities. In this case, the chatbot does not have to identify what the client wants, but instead, it will be the predefined dialogue tree that guides the conversation. This means that it will not be necessary to define intentions, but instead, entities or keywords will allow the bot to identify if it is collecting the appropriate information.
3. User instructions:
Once we have an overview of the dialogue and the keywords that the chatbot will “recognize”, it will be time to consider how the conversation will begin. It will be useful to warn the user about the functioning of the virtual agent, indicating that it is a robot and that it needs them to interact with it using concise phrases. Otherwise, users could begin to describe their problem using long text entries, making it difficult for the virtual agent to “understand” the content (that is, recognize the appropriate entities). The user may also be advised that he or she may be asked for pieces that he or she may have already mentioned in previous messages. Sometimes, in the same text entry, the client may report several pieces of data (e.g., the place where he or she had an anxiety attack and the people present). If the chatbot is not programmed to identify both data points in a single entry, it is likely that in other questions it will ask the user again for information that he or she has already given. If we do not warn the client that this may occur, he or she is more likely to become frustrated and leave the conversation without having finished the interview. 
4. Synthesis of the information collected by the chatbot:
Once the user has completed the interview with Alice, it will be stored on a secure server with restricted access. The information that identifies the user will be encrypted and separated from the rest of the information. In Alice’s case, specifically, we have designed the data output so that the whole conversation can be obtained with the relevant words (e.g., “sad”, “anxiety”, “fear”, “pain”, “ die” etc.). At the beginning of the conversation, a summary table will be generated that will collect these words in addition to the description of the user’s request. Thus, from first glance, one can know the subject of the case.
  

The information collected by Alice may be useful in deciding which professional to assign the case to within a team of psychologists. This work will be streamlined thanks to the when tool since it will not be necessary to spend time personally evaluating each case. Likewise, the situation will be avoided when the user starts therapy with a therapist other than the one who made the initial assessment. It must be said that the therapist assigned to the case will have the information collected by the chatbot, which will facilitate the analysis of the problem and the preparation of the face-to-face sessions. If this tool is used by one professional alone, the information collected will also help to plan the assessment and even make the first approach to explaining the problem.
5. Interdisciplinary Development:
From everything described above, it can be noted that collaboration among psychologists, computer engineers, and linguists is vital. Psychologists are responsible for indicating what the chatbot’s questions should be and the type of information that should be collected. In addition, they are responsible for pointing out the situations that require priority referral to a human psychologist or even, more specifically, to a specialized emergency service (e.g., the user presents suicidal ideation and a set plan). The engineers and linguists are responsible for designing and building the chatbot so that it can understand the user (identifying certain words in the text or voice input), guide the conversation, interact with language that is as natural as possible, etc. 
Limitations:
* Alice might be sensitive to changes in the wording of inputs and may provide different answers depending on the exact phrasing of a question. For instance, she may not know the answer to a question when asked one way, but be able to answer correctly when the same question is rephrased slightly.
* Instead of asking clarifying questions when faced with an ambiguous query, current large language models like Alice often try to guess what the user intended. This can sometimes lead to incorrect or misleading answers. Ideally, these models would be able to ask for clarification in such situations to improve the accuracy of their responses.


After Alice’s deployment, users are encouraged to please provide feedback. We are interested in any problematic model outputs and false positives/negatives from the external content filter. In particular, we would like to hear about any harmful results that could occur in real-world, non-adversarial conditions, as well as feedback that helps us identify and mitigate novel risks.
We are looking forward to applying the lessons learned from this release to the deployment of more advanced systems, as previous deployments have helped inform this one.
[1]
  

________________
[1] References: 
How to Create a Psychologist Chatbot
               A Psychologist Chatbot Developing Experience